{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eV2O-29T9gBF"
      },
      "source": [
        "# Basic RAG Implementation - Step by Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6i1iQkPkSAFa"
      },
      "source": [
        "## Set environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90p549_6nzWJ"
      },
      "source": [
        "**Run requirements**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_eZ-xL3SEQS"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet langchain langchain_community langchain-openai langchainhub faiss-cpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7j3vkmTn7dA"
      },
      "source": [
        "**Dev requirements**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIp_Uisrn--6"
      },
      "outputs": [],
      "source": [
        "! pip install --upgrade --quiet tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kLAEshFBsxL"
      },
      "source": [
        "**Environment Variables**\n",
        "\n",
        "The example below requires an OpenAI API key, so please create an account and a key if you don't have one yet.\n",
        "\n",
        "- [OpenAI docs](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)\n",
        "\n",
        "- [Video](https://www.youtube.com/watch?v=aVog4J6nIAU&pp=ygUOb3BlbmFpIGFwaSBrZXk%3D)\n",
        "\n",
        "\n",
        "The Langchain API key is not mandatory, but it allows monitoring the process with LangSmith.\n",
        "\n",
        "- [Langchain docs](https://docs.smith.langchain.com/setup#create-an-api-key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EX8CK4jwU8--",
        "outputId": "8dafdf7d-f4bb-47e4-9f26-a00ae93c9f15"
      },
      "outputs": [],
      "source": [
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "OPENAI_API_KEY = getpass('Please enter the secret value for OpenAI Key: ')\n",
        "os.environ['OPENAI_API_KEY']= OPENAI_API_KEY\n",
        "\n",
        "# Set them if you have a Langchain API key\n",
        "\n",
        "LANGCHAIN_API_KEY = getpass('Please enter the secret value for LangChain Key: ')\n",
        "os.environ['LANGCHAIN_API_KEY'] = LANGCHAIN_API_KEY\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBwPcFc9MFJB"
      },
      "source": [
        "## Star ExpertBot - Simple RAG Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VDrgleJ7dcg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "from langchain.storage import LocalFileStore\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "# ====================\n",
        "#      Indexing\n",
        "# ====================\n",
        "\n",
        "## Load knowledgebase\n",
        "loader = TextLoader(\"./stardust_serenade.txt\")\n",
        "knowledgebase = loader.load()\n",
        "\n",
        "## Split knowledgebase in chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100)\n",
        "chunks = text_splitter.split_documents(knowledgebase)\n",
        "\n",
        "## Embed knowledgebase\n",
        "embedder = OpenAIEmbeddings() # Default: model=text-embedding-ada-002\n",
        "store = LocalFileStore(\"/home/cache/\")\n",
        "\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    embedder, store, namespace=embedder.model\n",
        ")\n",
        "db_vector = FAISS.from_documents(chunks, cached_embedder)\n",
        "\n",
        "\n",
        "# ====================\n",
        "#      Retrieval\n",
        "# ====================\n",
        "\n",
        "retriever = db_vector.as_retriever()\n",
        "\n",
        "\n",
        "# =============================\n",
        "#     Retrieval and Generation\n",
        "# =============================\n",
        "\n",
        "retrieval_chain = (\n",
        "    (lambda x: x[\"input\"])\n",
        "    | retriever\n",
        "    | (lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs))\n",
        "    )\n",
        "\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "chat_model = ChatOpenAI() # Default: model=gpt-3.5-turbo, temperature=0.7\n",
        "\n",
        "\n",
        "simple_chain = prompt | chat_model | StrOutputParser()\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retrieval_chain , \"input\": RunnablePassthrough()}\n",
        "    | simple_chain\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stauOWTR2rix",
        "outputId": "43ef99df-07b7-403e-da58-f4a54dd493db"
      },
      "outputs": [],
      "source": [
        "# ====================\n",
        "#        Invoke\n",
        "# ====================\n",
        "question = (\n",
        "    \"What were the names of the three stars in the story, \"\\\n",
        "    \"and what were their unique characteristics?\"\n",
        ")\n",
        "\n",
        "result = rag_chain.invoke({\"input\": question})\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BVeMJPVJS_V"
      },
      "source": [
        "**Monitoring**\n",
        "\n",
        "Open Langsmith to see the workflow steps -> [LangSmith](https://smith.langchain.com/) ðŸ¦œ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIZY-wvR7iBt"
      },
      "source": [
        "### Step by Step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gdU4Lf5MWqF"
      },
      "source": [
        "#### **1. Load knowledgebase**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ypw1WdSzW0"
      },
      "source": [
        "Langchain allows us to load documents from many sources like markdown CSV and plane files, URLs, Git Hub, Google Drive, images, Data Dog, WhatsApp, etc. In this case, we are going to use a markdown file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CkHj53PKj2b",
        "outputId": "a5f1998f-9c06-48e1-eafc-5878017697c8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_core.documents import Document\n",
        "from typing import (List)\n",
        "\n",
        "\n",
        "def load_documents_from_plane_text(knowledgebase_file_path: str) -> List[Document]:\n",
        "        \"\"\"Load the knowledgebase from a plane text file\"\"\"\n",
        "\n",
        "        loader = TextLoader(knowledgebase_file_path)\n",
        "        documents = loader.load()\n",
        "\n",
        "        return documents\n",
        "\n",
        "knowledgebase = load_documents_from_plane_text(\"./stardust_serenade.txt\")\n",
        "print(f\"{knowledgebase = }\\n{len(knowledgebase) = }\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UwZbxFQ-o43"
      },
      "source": [
        "**Source:** [Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWGWi4rGNKxL"
      },
      "source": [
        "#### **2. Format knowledgebase**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4ghHD8NS4kq"
      },
      "source": [
        "Lanchain has different text splitters like `CharacterTextSplitter`, `MarkdownHeaderTextSplitter`, `RecursiveCharacterTextSplitter`, etc.\n",
        "\n",
        "The most important parameters when we set a splitter are:\n",
        "\n",
        "- **chunk_size:** It could represent either characters or tokens depending on the splitter type. The value depends on the context window supported by the embedding model.\n",
        "\n",
        "- **chunk_overlap:** It represents the number of characters the chunks will share if the splitter has to cut a paragraph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2jebzwFN9Zh",
        "outputId": "c10f07ae-b6d1-46aa-9252-fffde8ddde2f"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "def format_knowledgebase(knowledgebase: List[Document]) -> List[Document]:\n",
        "        \"\"\"Split the knowledgebase in chunks\"\"\"\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "        formatted_documents = text_splitter.split_documents(knowledgebase)\n",
        "\n",
        "        return formatted_documents\n",
        "\n",
        "chunks = format_knowledgebase(knowledgebase)\n",
        "print(f\"Number of chunks: {len(chunks)}\")\n",
        "print(f\"First chunk: {chunks[0]}\\n\")\n",
        "\n",
        "for n, chunk in enumerate(chunks):\n",
        "  print(f\"Chunk {1 + n}:\")\n",
        "  content = chunk.page_content\n",
        "  print(f\"Number of characters: {len(content)}\")\n",
        "\n",
        "  overlap = chunks[n - 1].page_content[-30:]\n",
        "\n",
        "  if overlap in content:\n",
        "    print(f\"There is an overlap\")\n",
        "  print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Sx8D6oK-zgu"
      },
      "source": [
        "**Source:** [Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLrdvrpPN-LS"
      },
      "source": [
        "#### **3. Embedding**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYhNwLUxME5P"
      },
      "source": [
        "**Count tokens**\n",
        "\n",
        "Requests to the OpenAI embeddings models API are billed based on the number of tokens in the input.\n",
        "\n",
        "The following example uses the cl100k_base encoding to count tokens because it is compatible with third-generation models like text-embedding-ada-002."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Guy6SkrNMPwN",
        "outputId": "8a44958b-621c-42a1-864b-d148d48c15b6"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n",
        "\n",
        "mini_knowledgebase = [\"Once upon a time, in the vast cosmic expanse, there existed three remarkable stars: Azure, Sunny, and Rosette.\",\n",
        "                      \"On the other side, they discovered a realm of Rainbow Starsâ€”each one a fusion of their colors.\"]\n",
        "num_tokens = num_tokens_from_string(mini_knowledgebase[0], \"cl100k_base\")\n",
        "print(f\"{num_tokens = }\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuv3tzHSSphh"
      },
      "source": [
        "**Embed Text**\n",
        "\n",
        "LangChain supports different embedding models. Here we use the `text-embedding-ada-002` model from OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vF3aqnwxF_fn"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embedder = OpenAIEmbeddings() # Default: text-embedding-ada-002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmyLTdoDTAFQ",
        "outputId": "49cedb5d-3b6e-4b79-f751-0f0e9dfbd06a"
      },
      "outputs": [],
      "source": [
        "mini_knowledgebase_vector = embedder.embed_documents(mini_knowledgebase)\n",
        "\n",
        "print(f\"{mini_knowledgebase_vector = }\")\n",
        "print(f\"{len(mini_knowledgebase_vector) = }\")\n",
        "print(f\"Vector's dimention: {len(mini_knowledgebase_vector[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDRXAfbQSt42"
      },
      "source": [
        "**Cache Backed Embeddings**\n",
        "\n",
        "Having embeddings in the cache will optimize the design, development and deployment, and if the model isn't free we'll save money."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZo4HkgzXfhN"
      },
      "outputs": [],
      "source": [
        "from langchain.storage import LocalFileStore\n",
        "from langchain.embeddings import CacheBackedEmbeddings\n",
        "\n",
        "store = LocalFileStore(\"./cache/\")\n",
        "\n",
        "cached_embedder = CacheBackedEmbeddings.from_bytes_store(\n",
        "    embedder, store, namespace=embedder.model\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LdwhArqVAjy"
      },
      "source": [
        "**Store Embeddings**\n",
        "\n",
        "Instead calling directly the embed_query method to create the embeddings, we generate them by defining the vector store.\n",
        "\n",
        "Let's see how to store the embddings using a cache."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34dMIvhZWXgj",
        "outputId": "404035a1-4e31-400c-fa80-01ffae09ea68"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "%time\n",
        "vector_store = FAISS.from_documents(chunks, cached_embedder) # If you are not using a chache use just the embedder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvPG36sGax7U"
      },
      "source": [
        "Now you could see the embedding files in /home/cache directory.\n",
        "\n",
        "If we create the vector_store again, it will take the embeddings form the cache directory and won't call the embedding model, make it the store process faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8wiid2gbR1L",
        "outputId": "7c2603bf-73b0-41e0-c1d4-9f7c593f7a15"
      },
      "outputs": [],
      "source": [
        "%time\n",
        "vector_store_2 = FAISS.from_documents(chunks, cached_embedder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COhM9ZME-9Y6"
      },
      "source": [
        "**Sources:**\n",
        "- [What are tokens and how to count them?](https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them#h_cd01d4bb9a)\n",
        "- [Tokenizer tool](https://platform.openai.com/tokenizer)\n",
        "- [Embedding Models](https://python.langchain.com/docs/integrations/text_embedding/)\n",
        "- [Cache Backed Embeddings](https://python.langchain.com/docs/modules/data_connection/text_embedding/caching_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23vSvEtZLGyM"
      },
      "source": [
        "#### **4. Retrive the context**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ7W1n8SzLxF"
      },
      "source": [
        "To get the relevant context, we need to create a retriver, but let's first see how  this process works manually and then how it works using the retriever."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGT9XvmPgrGn"
      },
      "source": [
        "**Calculate similarity manually**\n",
        "\n",
        "The Cosine Similarity is a measure recommended to find similar vectors. For OpenAI embeddings, \"1\" indicates identical vectors.\n",
        "\n",
        "Let's find the similarity between a question and two phrases manually.\n",
        "\n",
        "- **Question:** What were the names of the three stars in the story?\n",
        "- **Phrase 1:** Once upon a time, in the vast cosmic expanse, there existed three remarkable stars: Azure, Sunny, and Rosette.\n",
        "- **Phrase 2:** On the other side, they discovered a realm of Rainbow Starsâ€”each one a fusion of their colors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kLz_Jsum2tj"
      },
      "source": [
        "We need first create the embdding the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3Ns8a8Gm8WL"
      },
      "outputs": [],
      "source": [
        "question = \"What were the names of the three stars in the story?\"\n",
        "question_vector = embedder.embed_query(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxxjrEnvnAg9"
      },
      "source": [
        "Now we could run the cosine similarity and find the most similar phrase.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96hZYDfwgv9F",
        "outputId": "222da27a-1533-4ce5-8e2e-2eb5f925351d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "most_similar = \"\"\n",
        "max_similarity = 0\n",
        "\n",
        "for i, phrase_vector in enumerate(zip(mini_knowledgebase, mini_knowledgebase_vector)):\n",
        "\n",
        "  phrase, vector = phrase_vector\n",
        "\n",
        "  similarity = cosine_similarity(question_vector, vector)\n",
        "\n",
        "  tag = f\"phrase_{i + 1}\"\n",
        "  print(f\"{tag} similarity: {similarity}\")\n",
        "\n",
        "  if similarity > max_similarity:\n",
        "    most_similar = phrase\n",
        "    max_similarity = similarity\n",
        "\n",
        "print(f\"\\n{most_similar = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QdJJNTunXfJ"
      },
      "source": [
        "**Using a Retriever**\n",
        "\n",
        "The code bellow use the retriever to find the context to answer the question (the most similar two chunks).\n",
        "\n",
        "Langchaing supports many retriever types. In this case, we'll use the same vector store as the retriever (Vector store-backed retriever).\n",
        "\n",
        "Vector store-backed retriever has different types of search. In the example below, we'll use similarity search and specify the maximum chunks it must retrieve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgmVBD8t4qCS"
      },
      "outputs": [],
      "source": [
        "retriever = vector_store.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7CGhqeevkD4",
        "outputId": "2be2b193-7e26-4134-bd71-c8a341b7cb77"
      },
      "outputs": [],
      "source": [
        "context = retriever.get_relevant_documents(question)\n",
        "\n",
        "print(f\"{context = }\")\n",
        "print(f\"{len(context) = }\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xj4ieIBJ_fy9"
      },
      "source": [
        "**Sources:**\n",
        "- [Cosine similarity](https://platform.openai.com/docs/guides/embeddings/frequently-asked-questions)\n",
        "- [Retrievers Types](https://python.langchain.com/docs/modules/data_connection/retrievers/)\n",
        "- [Vector store-backed retriever](https://python.langchain.com/docs/modules/data_connection/retrievers/vectorstore)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJTPqekyyLqZ"
      },
      "source": [
        "#### **5. Generate the Answer**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zp1hIPYzFo5"
      },
      "source": [
        "As we have the context now, we need to use a LLM to respond the question according to it. But let's first see what the LLM responses withouth the context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HL89yL6YFVu"
      },
      "source": [
        "**Simple Chain**\n",
        "\n",
        "We'll LCEL (Lang Chain Expression Language) to build the chain. A simple chain is composed by:\n",
        "\n",
        "- A prompt template: The instruction we'll give to the LLM.\n",
        "- A LLM: The model we'll use to answer the question. In this case, it's ChatGPT.\n",
        "- An output parser: The parser we'll use to get the answer from the LLM response and format it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnoxoWAkYfwQ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"You are an expert in childrenâ€™s stories, please answer the following question.\"\n",
        "    \"\\nQuestion: {input}\"\n",
        "    )\n",
        "llm = ChatOpenAI()  # Default: Model=gpt-3.5-turbo Temperature=0.7\n",
        "output_parser = StrOutputParser()\n",
        "\n",
        "simple_chain = prompt | llm | output_parser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqakC0wgchfF",
        "outputId": "3750e4ae-59f8-4700-ac65-51694a8100c1"
      },
      "outputs": [],
      "source": [
        "# Test\n",
        "question = \"What were the names of the three stars in the story, and what were their unique characteristics?\"\n",
        "answer = simple_chain.invoke({\"input\": question})\n",
        "print(f\"{answer =}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8S0fIqlQFlM"
      },
      "source": [
        "Chains works in the way each element takes as input the output of the previous element. So we need be aware the inputs and outputs are compatible.\n",
        "\n",
        "The example below shows how to see the input and output schemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FxI-y1MeQ1VC",
        "outputId": "3dcfba3d-73d6-4502-e320-0bb00faad2f6"
      },
      "outputs": [],
      "source": [
        "prompt.output_schema.schema()[\"anyOf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cAzlkJMTMhM",
        "outputId": "3a0971f8-42a4-451e-f198-20cf2a82ec7e"
      },
      "outputs": [],
      "source": [
        "llm.input_schema.schema()[\"anyOf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcYzjXKlTPlY",
        "outputId": "488d5005-e58b-4571-f258-3b7004743c55"
      },
      "outputs": [],
      "source": [
        "llm.output_schema.schema()[\"anyOf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGmRx2ymU4ZV",
        "outputId": "ba0b267e-3345-4a58-efda-f99c1bbff62d"
      },
      "outputs": [],
      "source": [
        "output_parser.input_schema.schema()[\"anyOf\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BN57Q-aRVLx0",
        "outputId": "124414aa-c436-4420-c485-40e9ffce9124"
      },
      "outputs": [],
      "source": [
        "output_parser.output_schema.schema()[\"type\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mJ6AJMsgpIG"
      },
      "source": [
        "**RAG Chain**\n",
        "\n",
        "A RAG chain is composed by a simple chain and a retrieval chain.\n",
        "\n",
        "In this example the retrieval chain is compose by:\n",
        "- Input parser function\n",
        "- Retriever\n",
        "- context formatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X57P_DT4wo2R"
      },
      "outputs": [],
      "source": [
        "retrieval_chain = (\n",
        "    (lambda x: x[\"input\"])\n",
        "    | retriever\n",
        "    | (lambda docs: \"\\n\\n\".join(doc.page_content for doc in docs))\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOAHkZrKTVEi"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "\n",
        "Question: {input}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "chat_model = ChatOpenAI()\n",
        "\n",
        "\n",
        "simple_chain = prompt | chat_model | StrOutputParser()\n",
        "\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retrieval_chain , \"input\": RunnablePassthrough()}\n",
        "    | simple_chain\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y55u6F085p5T",
        "outputId": "77a39414-696e-40a3-8733-fdc422ef302d"
      },
      "outputs": [],
      "source": [
        "answer = rag_chain.invoke({\"input\": question})\n",
        "print(f\"{answer =}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUj6VzQDJygc"
      },
      "source": [
        "**Prebuilted Chains**\n",
        "\n",
        "Langchain has some prebuilted promps and chains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvbAvADjPhXb"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains.retrieval import create_retrieval_chain\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import hub\n",
        "\n",
        "\n",
        "prompt = hub.pull(\"langchain-ai/retrieval-qa-chat\")\n",
        "chat_model = ChatOpenAI()\n",
        "\n",
        "document_chain = create_stuff_documents_chain(chat_model, prompt)\n",
        "\n",
        "rag_chain = create_retrieval_chain(retriever, document_chain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqa58VifQOhL",
        "outputId": "49d2ec16-6db6-492c-a708-74a0243a3eb9"
      },
      "outputs": [],
      "source": [
        "answer = rag_chain.invoke({\"input\": question})[\"answer\"]\n",
        "print(f\"{answer =}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCRpARKO_sEn"
      },
      "source": [
        "**Sources**:\n",
        "\n",
        "- [LangChain Expression Language](https://python.langchain.com/docs/expression_language/get_started)\n",
        "- [Prompt](https://python.langchain.com/docs/modules/model_io/prompts/quick_start)\n",
        "- [Chat Model](https://python.langchain.com/docs/modules/model_io/chat/quick_start)\n",
        "- [Output Parser](https://python.langchain.com/docs/modules/model_io/output_parsers/quick_start)\n",
        "- [Chains](https://python.langchain.com/docs/modules/chains)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Yb9SDZLDANkN",
        "Owy9ZCiMPp_K",
        "aNgr6NumP1Jw",
        "6i1iQkPkSAFa",
        "9gdU4Lf5MWqF",
        "cWGWi4rGNKxL"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
